# =============================================================================
# Example Config: Stage 1 → Stage 2 Full Fine-tuning (Feature + Mel + STFT Loss)
# =============================================================================
# This config corresponds to Exp E (decompressor_only, AISHELL-1 ~25h).
# It trains Stage 1 (50Hz, 550 bps) then Stage 2 (25Hz, 275 bps) sequentially.
#
# Training script: train_stage1_50hz.py → train_stage2_25hz.py
# Results (AISHELL-1 test, 2000 utts):
#   Stage 1 (50Hz): dCER = 6.17%, MOS_Q = 2.96
#   Stage 2 (25Hz): dCER = 13.33%, MOS_Q = 2.74
# Results (LibriSpeech test-clean, 2000 utts):
#   Stage 1 (50Hz): dWER = 3.87%
#   Stage 2 (25Hz): dWER = 30.57%  ← English degrades without bilingual data
#
# NOTE: Replace all paths marked with <YOUR_...> before training.

# -----------------------------------------------------------------------------
# Paths — CHANGE THESE TO YOUR OWN PATHS
# -----------------------------------------------------------------------------
paths:
  base_dir: "<YOUR_PROJECT_DIR>"             # e.g., /home/user/FocalCodec_Release
  focalcodec_dir: "<YOUR_PROJECT_DIR>/focalcodec"
  model_cache_dir: "<YOUR_MODEL_CACHE_DIR>"  # HuggingFace model cache
  asr_cache_dir: "<YOUR_MODEL_CACHE_DIR>"
  output_dir: "<YOUR_PROJECT_DIR>/output"    # where checkpoints are saved
  inference_dir: "<YOUR_PROJECT_DIR>/inference/my_exp"

# -----------------------------------------------------------------------------
# Data — CHANGE THESE TO YOUR OWN DATASET PATHS
# -----------------------------------------------------------------------------
data:
  audio_base_path: "<YOUR_DATASET_ROOT>"     # root dir of your audio files
  train_csv: "<YOUR_PROJECT_DIR>/data/train_split.csv"
  val_csv: "<YOUR_PROJECT_DIR>/data/val_split.csv"
  test_csv: "<YOUR_PROJECT_DIR>/data/test_split.csv"

# CSV format (tab or comma separated):
#   filepath,duration
#   /path/to/audio.wav,3.21
# The filepath can be relative to audio_base_path or absolute.

# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------
model:
  base_model: "lucadellalib/focalcodec_50hz_2k_causal"  # pretrained HuggingFace model
  teacher_model: "lucadellalib/focalcodec_25hz"
  codebook_size: 2048

# -----------------------------------------------------------------------------
# Stage 1: 50Hz (550 bps) — Fine-tune from pretrained
# -----------------------------------------------------------------------------
stage1:
  # "decompressor_only" trains only decompressor (76M/249M params, recommended)
  # "both_ste" trains both compressor+decompressor (118M/249M params)
  train_mode: "decompressor_only"

  batch_size: 256        # reduce to 128 if OOM on smaller GPUs
  chunk_duration: 3.0    # seconds per audio chunk
  overlap: 0.5
  max_chunks: -1         # -1 = use all chunks

  # Optimizer
  learning_rate: 5.0e-04
  weight_decay: 0.01
  betas: [0.9, 0.98]

  # Scheduler: ReduceLROnPlateau
  scheduler_type: "plateau"
  num_epochs: 10000
  scheduler_factor: 0.5
  scheduler_patience: 5
  scheduler_threshold: 0.001
  scheduler_cooldown: 2
  scheduler_min_lr: 1.0e-06

  # Warmup
  warmup_epochs: 5
  warmup_start_lr: 1.0e-06

  # Gradient clipping
  gradient_clip: 5.0

  # Early stopping
  patience: 20
  min_delta: 0.0001

  # Loss weights (Feature:Mel:STFT = 1:5:2 is the recommended balance)
  weight_feature: 1.0   # WavLM feature MSE (semantic preservation)
  weight_time: 0.0
  use_mel_loss: true
  weight_mel: 5.0       # Mel-spectrogram loss (timbre/prosody)
  use_stft_loss: true
  weight_stft: 2.0      # Multi-resolution STFT loss (high-freq details)

  enable_codebook_monitor: true
  log_interval: 10

# -----------------------------------------------------------------------------
# Stage 2: 25Hz (275 bps) — Add 4th compressor/decompressor layer
# -----------------------------------------------------------------------------
stage2:
  batch_size: 256
  chunk_duration: 3.0
  overlap: 0.5
  max_chunks: -1

  # Dual learning rate: new layer needs higher LR to warm up
  learning_rate_new: 5.0e-04   # new 4th layer
  learning_rate_old: 5.0e-05   # inherited layers from Stage 1
  num_epochs: 10000
  warmup_epochs: 10
  gradient_clip: 5.0

  scheduler_type: "plateau"
  scheduler_factor: 0.5
  scheduler_patience: 8
  scheduler_threshold: 0.001
  scheduler_cooldown: 3
  scheduler_min_lr: 1.0e-06

  patience: 30
  min_delta: 0.0001

  weight_feature: 1.0
  weight_time: 0.0
  use_mel_loss: true
  weight_mel: 5.0
  use_stft_loss: true
  weight_stft: 2.0

  enable_codebook_monitor: true
  log_interval: 10

# -----------------------------------------------------------------------------
# Stage 3: 12.5Hz (137.5 bps) — Add 5th compressor/decompressor layer
# -----------------------------------------------------------------------------
stage3:
  batch_size: 256
  chunk_duration: 3.0
  overlap: 0.5
  max_chunks: -1

  learning_rate_new: 5.0e-04
  learning_rate_old: 5.0e-05
  num_epochs: 10000
  warmup_epochs: 10
  gradient_clip: 5.0

  scheduler_type: "plateau"
  scheduler_factor: 0.5
  scheduler_patience: 8
  scheduler_threshold: 0.001
  scheduler_cooldown: 3
  scheduler_min_lr: 1.0e-06

  patience: 30
  min_delta: 0.0001

  weight_feature: 1.0
  weight_time: 0.0
  use_mel_loss: true
  weight_mel: 5.0
  use_stft_loss: true
  weight_stft: 2.0

  enable_codebook_monitor: true
  log_interval: 10

device: "cuda"
num_workers: 4   # match your SLURM cpus-per-task
pin_memory: true
